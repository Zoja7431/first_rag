import sys
from pathlib import Path
import csv
from typing import List, Dict, Tuple
import statistics

# sys.path fix
sys.path.insert(0, str(Path(__file__).parent))

from src.config import get_config
from src.data_loader import get_pdf_chunker
from src.embeddings import get_embeddings_manager
from src.vector_store import get_vector_store_manager
import numpy as np
from langchain_core.documents import Document

# ============================================================================
# –ê–ù–ê–õ–ò–ó –ß–ê–ù–ö–û–í
# ============================================================================

class ChunkAnalyzer:
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–æ–≤"""
    
    def __init__(self):
        self.config = get_config()
        self.chunker = get_pdf_chunker()
        self.chunks: List[Document] = []
        self.stats: Dict = {}
    
    def load_pdf_chunks(self) -> List[Document]:
        """–ó–∞–≥—Ä—É–∂–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ –≤—Å–µ—Ö PDF"""
        pdf_dir = Path(self.config.data.pdf_path)
        pdf_files = list(pdf_dir.glob("*.pdf"))[:1]  # –ü–µ—Ä–≤—ã–π PDF –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        if not pdf_files:
            print("‚ùå –ù–µ—Ç PDF —Ñ–∞–π–ª–æ–≤!")
            return []
        
        print(f"üìö –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º PDF: {pdf_files[0].name}")
        self.chunks = self.chunker.load_and_chunk_pdf(str(pdf_files[0]))
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.chunks)} —á–∞–Ω–∫–æ–≤\n")
        return self.chunks
    
    def analyze_chunk_sizes(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤"""
        if not self.chunks:
            return {}
        
        char_counts = [len(c.page_content) for c in self.chunks]
        token_estimates = [len(c.page_content) // 4 for c in self.chunks]  # –ì—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞
        
        stats = {
            'total_chunks': len(self.chunks),
            'total_chars': sum(char_counts),
            'total_tokens_est': sum(token_estimates),
            'char_mean': statistics.mean(char_counts),
            'char_median': statistics.median(char_counts),
            'char_stdev': statistics.stdev(char_counts) if len(char_counts) > 1 else 0,
            'char_min': min(char_counts),
            'char_max': max(char_counts),
            'token_mean_est': statistics.mean(token_estimates),
        }
        
        return stats
    
    def analyze_chunk_content_quality(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —á–∞–Ω–∫–æ–≤"""
        if not self.chunks:
            return {}
        
        # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤—ã—Ö 10 —á–∞–Ω–∫–æ–≤
        sample_chunks = self.chunks[:10]
        
        quality_metrics = {
            'sample_size': len(sample_chunks),
            'chunks_with_content': 0,
            'chunks_under_50_chars': 0,
            'chunks_over_1000_chars': 0,
            'has_cyrillic': 0,
            'has_numbers': 0,
            'has_special_chars': 0,
        }
        
        for chunk in sample_chunks:
            text = chunk.page_content
            
            if text.strip():
                quality_metrics['chunks_with_content'] += 1
            
            if len(text) < 50:
                quality_metrics['chunks_under_50_chars'] += 1
            
            if len(text) > 1000:
                quality_metrics['chunks_over_1000_chars'] += 1
            
            if any('\u0400' <= char <= '\u04FF' for char in text):
                quality_metrics['has_cyrillic'] += 1
            
            if any(char.isdigit() for char in text):
                quality_metrics['has_numbers'] += 1
            
            if any(char in '.,;:!?-' for char in text):
                quality_metrics['has_special_chars'] += 1
        
        return quality_metrics
    
    def show_sample_chunks(self, n: int = 3):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤"""
        print("\n" + "="*80)
        print("üìÑ –ü–†–ò–ú–ï–†–´ –ß–ê–ù–ö–û–í (–ø–µ—Ä–≤—ã–µ 3)")
        print("="*80)
        
        for i, chunk in enumerate(self.chunks[:n]):
            text = chunk.page_content
            meta = chunk.metadata
            
            print(f"\n{'‚îÄ'*80}")
            print(f"–ß–∞–Ω–∫ #{meta.get('chunk_id', i)} –∏–∑ –∫–Ω–∏–≥–∏ '{meta.get('book', '?')}' —Å—Ç—Ä.{meta.get('page', '?')}")
            print(f"–†–∞–∑–º–µ—Ä: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ ({len(text)//4} —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–∏–º–µ—Ä–Ω–æ)")
            print(f"{'‚îÄ'*80}")
            print(f"{text[:300]}...")
            print(f"Metadata: {meta}")
    
    def save_statistics_to_csv(self, filename: str = "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞_—á–∞–Ω–∫–æ–≤.csv"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ –≤ CSV"""
        if not self.chunks:
            print("‚ùå –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!")
            return
        
        rows = []
        for chunk in self.chunks:
            text = chunk.page_content
            meta = chunk.metadata
            
            rows.append({
                'chunk_id': meta.get('chunk_id', ''),
                'book': meta.get('book', ''),
                'page': meta.get('page', ''),
                'chunk_index': meta.get('chunk_index', ''),
                'char_count': len(text),
                'token_count_est': len(text) // 4,
                'word_count': len(text.split()),
                'has_newline': '\n' in text,
                'preview': text[:100].replace('\n', ' '),
            })
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=rows[0].keys())
            writer.writeheader()
            writer.writerows(rows)
        
        print(f"‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filename}")
    
    def print_statistics(self):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ –∫–æ–Ω—Å–æ–ª—å"""
        size_stats = self.analyze_chunk_sizes()
        quality_stats = self.analyze_chunk_content_quality()
        
        print("\n" + "="*80)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ß–ê–ù–ö–û–í")
        print("="*80)
        
        print(f"\nüìà –†–ê–ó–ú–ï–†–´ –ß–ê–ù–ö–û–í:")
        print(f"  –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {size_stats.get('total_chunks', 0)}")
        print(f"  –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {size_stats.get('total_chars', 0):,}")
        print(f"  –ü—Ä–∏–º–µ—Ä–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {size_stats.get('total_tokens_est', 0):,}")
        print(f"  –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {size_stats.get('char_mean', 0):.1f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  –ú–µ–¥–∏–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {size_stats.get('char_median', 0):.1f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  –°—Ç–¥. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {size_stats.get('char_stdev', 0):.1f}")
        print(f"  –ú–∏–Ω–∏–º—É–º: {size_stats.get('char_min', 0)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  –ú–∞–∫—Å–∏–º—É–º: {size_stats.get('char_max', 0)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        print(f"\n‚úÖ –ö–ê–ß–ï–°–¢–í–û –°–û–î–ï–†–ñ–ò–ú–û–ì–û (–ø–µ—Ä–≤—ã–µ 10 —á–∞–Ω–∫–æ–≤):")
        print(f"  –° —Å–æ–¥–µ—Ä–∂–∏–º—ã–º: {quality_stats.get('chunks_with_content', 0)}/10")
        print(f"  <50 —Å–∏–º–≤–æ–ª–æ–≤: {quality_stats.get('chunks_under_50_chars', 0)}/10 (–ø–ª–æ—Ö–æ)")
        print(f"  >1000 —Å–∏–º–≤–æ–ª–æ–≤: {quality_stats.get('chunks_over_1000_chars', 0)}/10 (–º–æ–∂–µ—Ç –±—ã—Ç—å –º–Ω–æ–≥–æ)")
        print(f"  –° –∫–∏—Ä–∏–ª–ª–∏—Ü–µ–π: {quality_stats.get('has_cyrillic', 0)}/10")
        print(f"  –° —Ü–∏—Ñ—Ä–∞–º–∏: {quality_stats.get('has_numbers', 0)}/10")
        print(f"  –° –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π: {quality_stats.get('has_special_chars', 0)}/10")
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        avg_size = size_stats.get('char_mean', 0)
        if 200 <= avg_size <= 800:
            quality = "‚úÖ –û–¢–õ–ò–ß–ù–û"
        elif 100 <= avg_size <= 1500:
            quality = "üü° –•–û–†–û–®–û"
        else:
            quality = "‚ùå –ü–õ–û–•–û"
        
        print(f"\nüéØ –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê: {quality}")
        print(f"  –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 200-800 —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  –¢–µ–∫—É—â–∏–π —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_size:.0f} —Å–∏–º–≤–æ–ª–æ–≤")


# ============================================================================
# –ê–ù–ê–õ–ò–ó –≠–ú–ë–ï–î–î–ò–ù–ì–û–í
# ============================================================================

class EmbeddingsAnalyzer:
    """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    
    def __init__(self):
        self.config = get_config()
        try:
            self.embeddings_manager = get_embeddings_manager()
            self.embeddings_available = True
        except Exception as e:
            print(f"‚ö†Ô∏è Embeddings –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã: {e}")
            self.embeddings_available = False
    
    def test_embeddings(self, texts: List[str] = None):
        """–¢–µ—Å—Ç–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        if not self.embeddings_available:
            print("‚ö†Ô∏è Embeddings –º–µ–Ω–µ–¥–∂–µ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return
        
        if texts is None:
            texts = [
                "–ü—Å–∏—Ö–æ–ª–æ–≥–∏—è –∏–∑—É—á–∞–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ —á–µ–ª–æ–≤–µ–∫–∞",
                "–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç —Ç–µ–∫—Å—Ç –≤ –≤–µ–∫—Ç–æ—Ä—ã",
                "Qdrant —Ö—Ä–∞–Ω–∏—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ",
            ]
        
        print("\n" + "="*80)
        print("üß† –ê–ù–ê–õ–ò–ó –≠–ú–ë–ï–î–î–ò–ù–ì–û–í")
        print("="*80)
        
        try:
            embeddings = self.embeddings_manager.embed_texts(texts)
            
            print(f"\nüìä –ú–æ–¥–µ–ª—å: {self.config.embeddings.model_name}")
            print(f"üìä –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.config.embeddings.device}")
            print(f"üìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.config.embeddings.embedding_dim}")
            
            for i, (text, emb) in enumerate(zip(texts, embeddings)):
                emb_array = np.array(emb)
                print(f"\n‚úÖ –¢–µ–∫—Å—Ç {i+1}: '{text[:60]}...'")
                print(f"   –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞: {len(emb_array)}")
                print(f"   –ù–æ—Ä–º–∞ (L2): {np.linalg.norm(emb_array):.4f}")
                print(f"   –ü–µ—Ä–≤—ã–µ 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {emb_array[:5]}")
            
            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏
            if len(embeddings) >= 2:
                emb_arrays = [np.array(e) for e in embeddings]
                similarity = np.dot(emb_arrays[0], emb_arrays[1])
                print(f"\nüîó –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–º 1 –∏ 2: {similarity:.4f}")
                print(f"   (1.0 = –∏–¥–µ–Ω—Ç–∏—á–Ω—ã, 0.0 = —Ä–∞–∑–Ω—ã–µ, -1.0 = –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã)")
        
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")


# ============================================================================
# –ê–ù–ê–õ–ò–ó VECTOR STORE
# ============================================================================

class VectorStoreAnalyzer:
    """–ê–Ω–∞–ª–∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.config = get_config()
        try:
            self.vs_manager = get_vector_store_manager()
            self.available = True
        except Exception as e:
            print(f"‚ö†Ô∏è Vector Store –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            self.available = False
    
    def check_collection_status(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant"""
        if not self.available:
            print("‚ö†Ô∏è Vector Store –º–µ–Ω–µ–¥–∂–µ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return
        
        print("\n" + "="*80)
        print("üì¶ –°–¢–ê–¢–£–° VECTOR STORE (QDRANT)")
        print("="*80)
        
        try:
            stats = self.vs_manager.get_collection_stats()
            
            print(f"\n‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è: {stats.get('collection_name', '?')}")
            print(f"   URL: {self.config.qdrant.full_url}")
            print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ (points): {stats.get('points_count', 0)}")
            print(f"   –í–µ–∫—Ç–æ—Ä–æ–≤: {stats.get('vectors_count', 0)}")
            print(f"   –°–µ–≥–º–µ–Ω—Ç–æ–≤: {stats.get('segments_count', 0)}")
            
            if stats.get('points_count', 0) > 0:
                print(f"\nüéâ –î–æ–∫—É–º–µ–Ω—Ç—ã –£–ñ–ï –ó–ê–ì–†–£–ñ–ï–ù–´ –≤ vector store!")
            else:
                print(f"\n‚ö†Ô∏è Vector store –ü–£–°–¢. –î–æ–∫—É–º–µ–Ω—Ç—ã –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
                print(f"   –ò—Å–ø–æ–ª—å–∑—É–π: vector_store_manager.add_documents(chunks)")
        
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å—Ç–∞—Ç—É—Å–∞: {e}")
    
    def show_add_chunks_example(self):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∫ –¥–æ–±–∞–≤–∏—Ç—å —á–∞–Ω–∫–∏ –≤ vector store"""
        print("\n" + "="*80)
        print("üíæ –ö–ê–ö –î–û–ë–ê–í–ò–¢–¨ –ß–ê–ù–ö–ò –í VECTOR STORE")
        print("="*80)
        
        example = """
from src.data_loader import get_pdf_chunker
from src.vector_store import get_vector_store_manager

# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —á–∞–Ω–∫–∏
chunker = get_pdf_chunker()
chunks = chunker.load_multiple(['path/to/pdf1.pdf', 'path/to/pdf2.pdf'])

# 2. –î–æ–±–∞–≤–ª—è–µ–º –≤ vector store
vs_manager = get_vector_store_manager()
ids = vs_manager.add_documents(chunks)

print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(ids)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤!")

# 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å
stats = vs_manager.get_collection_stats()
print(f"–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ vector store: {stats['points_count']}")

# 4. –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É
results = vs_manager.search("–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è", k=5)
for doc in results:
    print(f"‚úÖ {doc.page_content[:100]}...")
"""
        print(example)


# ============================================================================
# MAIN
# ============================================================================

def main():
    print("üîç –ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó –°–ò–°–¢–ï–ú–´ –ß–ê–ù–ö–ò–†–û–í–ê–ù–ò–Ø –ò –≠–ú–ë–ï–î–î–ò–ù–ì–û–í\n")
    
    # 1. –ê–Ω–∞–ª–∏–∑ —á–∞–Ω–∫–æ–≤
    chunk_analyzer = ChunkAnalyzer()
    chunk_analyzer.load_pdf_chunks()
    chunk_analyzer.print_statistics()
    chunk_analyzer.show_sample_chunks(n=3)
    chunk_analyzer.save_statistics_to_csv()
    
    # 2. –ê–Ω–∞–ª–∏–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    emb_analyzer = EmbeddingsAnalyzer()
    emb_analyzer.test_embeddings()
    
    # 3. –°—Ç–∞—Ç—É—Å vector store
    vs_analyzer = VectorStoreAnalyzer()
    vs_analyzer.check_collection_status()
    vs_analyzer.show_add_chunks_example()
    
    print("\n" + "="*80)
    print("‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–Å–ù")
    print("="*80)


if __name__ == "__main__":
    main()