"""
src/rag_graph.py â€” LangGraph RAG: retrieve â†’ generate (LLM version)
ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹, Ñ‡Ğ¸ÑÑ‚Ñ‹Ğ¹, Ğ³Ğ¾Ñ‚Ğ¾Ğ² Ğº FastAPI
"""

from typing import TypedDict, Annotated, Sequence
from operator import add
import logging

from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.graph import StateGraph, END

from src.config import get_config
from src.vector_store import get_vector_store_manager
from src.llm import get_llm

logger = logging.getLogger(__name__)

class GraphState(TypedDict):
    question: str
    context: Annotated[Sequence[Document], add]
    response: str
    messages: Annotated[Sequence[AIMessage | HumanMessage], add]

def retrieve(state: GraphState) -> GraphState:
    """ğŸ” ĞŸĞ¾Ğ¸ÑĞº Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² Ğ² Qdrant"""
    vs = get_vector_store_manager()
    docs = vs.search(state["question"], k=6)
    logger.debug(f"Found {len(docs)} chunks for '{state['question'][:50]}...'")
    return {"context": docs}

def generate(state: GraphState) -> GraphState:
    """ğŸ¤– Groq LLM Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ"""
    config = get_config()
    context = "\n\n".join([f"[{i+1}] {doc.page_content}" 
                          for i, doc in enumerate(state["context"])])
    
    prompt_text = f"""{config.prompt.system_role}

ĞšĞĞĞ¢Ğ•ĞšĞ¡Ğ¢ Ğ˜Ğ— ĞšĞĞ˜Ğ“:
{context}

Ğ’ĞĞŸĞ ĞĞ¡: {state["question"]}

ĞĞ¢Ğ’Ğ•Ğ¢:"""
    
    llm = get_llm()  # llama-3.1-8b Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ
    
    try:
        response = llm.invoke(prompt_text)
        return {
            "response": response.content,
            "messages": [HumanMessage(content=state["question"]), 
                        AIMessage(content=response.content)]
        }
    except Exception as e:
        print(f"âŒ LLM Error: {e}")
        return {"response": f"ĞÑˆĞ¸Ğ±ĞºĞ° LLM: {str(e)}"}


# ğŸ—ï¸ Ğ“Ñ€Ğ°Ñ„
workflow = StateGraph(GraphState)
workflow.add_node("retrieve", retrieve)
workflow.add_node("generate", generate)
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)

rag_app = workflow.compile()

def ask(question: str, stream=False) -> str:
    """ğŸ¯ Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ RAG Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ"""
    result = rag_app.invoke({"question": question})
    return result["response"]
