"""
embeddings.py - –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç HuggingFace Sentence Transformers –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
"""

import logging
from typing import Optional
import numpy as np
from langchain_huggingface import HuggingFaceEmbeddings
from src.config import EmbeddingsConfig, get_config

logger = logging.getLogger(__name__)


class EmbeddingsManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
    
    _instance: Optional['EmbeddingsManager'] = None
    _embeddings_model: Optional[HuggingFaceEmbeddings] = None
    
    def __new__(cls):
        """Singleton –ø–∞—Ç—Ç–µ—Ä–Ω"""
        if cls._instance is None:
            cls._instance = super(EmbeddingsManager, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if self._embeddings_model is None:
            self._load_embeddings()
    
    def _load_embeddings(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        config = get_config()
        emb_config = config.embeddings
        
        logger.info(f"Loading embeddings model: {emb_config.model_name}")
        
        try:
            self._embeddings_model = HuggingFaceEmbeddings(
                model_name=emb_config.model_name,
                model_kwargs={
                    'device': emb_config.device,
                    **emb_config.model_kwargs
                },
                encode_kwargs={
                    'normalize_embeddings': True
                }
            )
            
            logger.info(f"‚úÖ Embeddings model loaded successfully")
            logger.info(f"   Model: {emb_config.model_name}")
            logger.info(f"   Device: {emb_config.device}")
            logger.info(f"   Embedding dimension: {emb_config.embedding_dim}")
            
        except Exception as e:
            logger.error(f"Failed to load embeddings model: {e}")
            raise
    
    def get_embeddings(self) -> HuggingFaceEmbeddings:
        """
        –ü–æ–ª—É—á–∏—Ç—å –æ–±—ä–µ–∫—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        
        Returns:
            HuggingFaceEmbeddings: –û–±—ä–µ–∫—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
        """
        if self._embeddings_model is None:
            self._load_embeddings()
        return self._embeddings_model
    
    def embed_text(self, text: str) -> np.ndarray:
        """
        –°–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        
        Returns:
            np.ndarray: –í–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ embedding_dim
        """
        if self._embeddings_model is None:
            self._load_embeddings()
        
        embedding = self._embeddings_model.embed_query(text)
        return np.array(embedding)
    
    def embed_texts(self, texts: list[str]) -> list[np.ndarray]:
        """
        –°–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
        
        Returns:
            list[np.ndarray]: –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        if self._embeddings_model is None:
            self._load_embeddings()
        
        embeddings = self._embeddings_model.embed_documents(texts)
        return [np.array(emb) for emb in embeddings]
    
    def get_embedding_dim(self) -> int:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        
        Returns:
            int: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        """
        config = get_config()
        return config.embeddings.embedding_dim


def get_embeddings_manager() -> EmbeddingsManager:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å–∏–Ω–≥–ª—Ç–æ–Ω –º–µ–Ω–µ–¥–∂–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    
    Returns:
        EmbeddingsManager: –ú–µ–Ω–µ–¥–∂–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    """
    return EmbeddingsManager()


def get_embeddings_model() -> HuggingFaceEmbeddings:
    """
    –ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å LangChain)
    
    Returns:
        HuggingFaceEmbeddings: –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    """
    return get_embeddings_manager().get_embeddings()


if __name__ == "__main__":
    import logging
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # –¢–µ—Å—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    manager = get_embeddings_manager()
    
    # –¢–µ—Å—Ç –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    test_text = "–ü—Å–∏—Ö–æ–ª–æ–≥–∏—è - —ç—Ç–æ –Ω–∞—É–∫–∞ –æ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ –∏ –ø—Å–∏—Ö–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö"
    embedding = manager.embed_text(test_text)
    
    print(f"\nüìä –¢–µ—Å—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:")
    print(f"–¢–µ–∫—Å—Ç: {test_text}")
    print(f"–†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {len(embedding)}")
    print(f"–ü–µ—Ä–≤—ã–µ 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {embedding[:5]}")
    print(f"–ù–æ—Ä–º–∞ –≤–µ–∫—Ç–æ—Ä–∞: {np.linalg.norm(embedding):.4f}")
    
    # –¢–µ—Å—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    texts = [
        "–ü—Å–∏—Ö–æ–ª–æ–≥–∏—è –∏–∑—É—á–∞–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ —á–µ–ª–æ–≤–µ–∫–∞",
        "–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç —Ç–µ–∫—Å—Ç –≤ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã"
    ]
    
    embeddings = manager.embed_texts(texts)
    print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(embeddings)}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    similarity = np.dot(embeddings[0], embeddings[1])
    print(f"–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–∞–º–∏: {similarity:.4f}")